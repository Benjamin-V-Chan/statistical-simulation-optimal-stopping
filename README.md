# statistical-simulation-optimal-stopping

## Project Overview

The **Optimal Stopping Problem**, also known as the "Secretary Problem," explores the challenge of determining the best moment to stop observing a sequence of options and make a selection, given the goal of maximizing the probability of choosing the optimal option.

This simulation models a sequence of randomly generated payoff values and evaluates the performance of various stopping strategies. The mathematical foundation includes:

### Mathematical Framework

Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space and let $X_1, X_2, \dots, X_n$ be i.i.d. (independent and identically distributed) real-valued random variables with continuous cumulative distribution function (CDF) $F$ and density $f$ (continuity ensures $\mathbb{P}(X_i = X_j)=0$ for $i\neq j$, so the maximum is almost surely unique). The decision-maker observes the sequence sequentially and must choose exactly one index.

Define the **natural filtration** (the information revealed up to time $t$) by
$$\mathcal{F}_t := \sigma(X_1,\dots,X_t), \qquad t=1,\dots,n,$$
where $\sigma(\cdot)$ denotes the $\sigma$-algebra generated by the random variables.

Define:

* $T$ as the stopping rule: formally, a (finite-horizon) **stopping time** $T:\Omega\to{1,2,\dots,n}$ such that ${T=t}\in\mathcal{F}_t$ for all $t$ (i.e., the decision to stop at time $t$ can only depend on what has been observed up to time $t$).

* $P(\text{success})$ as the probability of selecting the global maximum:
  $$P(\text{success}) := \mathbb{P}!\left(X_T = \max_{1\le i\le n} X_i\right).$$

A key structural fact is that, for the objective “pick the overall maximum,” the problem is **distribution-free** under the i.i.d. continuity assumption. The mechanism is the **probability integral transform**: define
$$U_i := F(X_i).$$
Then $U_1,\dots,U_n$ are i.i.d. $\text{Unif}(0,1)$ random variables, and since $F$ is strictly increasing almost surely on the support (under continuity), we have the equivalence
$$X_T = \max_{1\le i\le n} X_i \quad\Longleftrightarrow\quad U_T = \max_{1\le i\le n} U_i.$$
So maximizing $\mathbb{P}(X_T=\max_i X_i)$ is equivalent to maximizing $\mathbb{P}(U_T=\max_i U_i)$, which depends only on relative order (ranks), not on the specific form of $F$.

#### Strategy and Optimality

The classical optimal strategy is a **threshold (cutoff) record rule**: choose an integer cutoff $k\in{0,1,\dots,n-1}$, reject the first $k$ observations, then accept the first subsequent observation that is a **record** (i.e., larger than all previously observed values). Formally, define the running maximum
$$M_t := \max_{1\le i\le t} X_i,$$
and the stopping time for cutoff $k$ by
$$T_k := \inf{t\in{k+1,\dots,n}: X_t = M_t},$$
with the convention that if no record occurs after $k$, then $T_k:=n$ (you must choose something by the horizon).

The success probability of this cutoff strategy has an exact closed form. Let $H_m$ denote the $m$-th **harmonic number**:
$$H_m := \sum_{j=1}^{m}\frac{1}{j}, \qquad H_0:=0.$$
Then:
$$P_k(n) ;:=; \mathbb{P}!\left(X_{T_k} = \max_{1\le i\le n}X_i\right)
;=;\sum_{j=k+1}^{n}\frac{1}{n}\cdot\frac{k}{j-1}
;=;\frac{k}{n}\sum_{j=k+1}^{n}\frac{1}{j-1}
;=;\frac{k}{n}\Bigl(H_{n-1}-H_{k-1}\Bigr).$$

In this expression:

* $H_{n-1}-H_{k-1}=\sum_{m=k}^{n-1}\frac{1}{m}$ is the tail of the harmonic series, which behaves like a logarithm for large $n$.
* The factor $\frac{1}{n}$ comes from symmetry: the global maximum’s position is uniformly distributed over ${1,\dots,n}$.
* The factor $\frac{k}{j-1}$ is the probability that, conditioned on the maximum occurring at position $j$, the best among the first $j-1$ observations occurred within the first $k$ (which is exactly what prevents the algorithm from stopping before $j$).

A concise proof of the formula is:

1. Let $J$ be the (a.s. unique) index of the global maximum. By exchangeability, $\mathbb{P}(J=j)=1/n$.
2. If $J=j\le k$, the cutoff strategy cannot succeed (it never selects in the first $k$).
3. If $J=j>k$, the strategy succeeds iff the maximum among the first $j-1$ observations lies in ${1,\dots,k}$. By symmetry within the first $j-1$ positions, that probability is $k/(j-1)$.
4. Sum over $j=k+1,\dots,n$.

#### Why the $1/e$ Rule Works

The $1/e$ rule emerges from optimizing $P_k(n)$ over $k$ and taking the large- $n$ limit. The explanation below is both structural (why threshold rules are optimal) and analytic (why the optimizer is near $n/e$).

1. **Exploration Phase**:
   During the first $k$ steps, the algorithm observes but never selects. This is not about estimating $F$ (the distribution-free reduction shows $F$ is irrelevant for the success event); instead, it is about establishing a **benchmark record level**: the maximum of the first $k$ items. Any future selection must exceed this benchmark to have a meaningful chance of being the global maximum.

   A crucial admissibility reduction: selecting a non-record can never win, because if $X_t<M_{t-1}$ then $X_t$ is already beaten by a previously seen value and thus cannot be the overall maximum. Therefore, any optimal stopping time can be assumed (without loss of optimality) to stop only at record times.

2. **Exploitation Phase**:
   After the cutoff, the algorithm stops at the first record. The success probability is
   $$P_k(n)=\frac{k}{n}\Bigl(H_{n-1}-H_{k-1}\Bigr).$$

   To see the asymptotic shape, define the cutoff proportion $p:=k/n$ and use the classical asymptotic expansion of harmonic numbers (where $\gamma$ is the Euler–Mascheroni constant):
   $$H_m=\log m+\gamma+\frac{1}{2m}-\frac{1}{12m^2}+O!\left(\frac{1}{m^4}\right).$$
   Plugging into $P_k(n)$ (with $k=\lfloor pn\rfloor$) yields
   $$H_{n-1}-H_{k-1}
   =\log!\left(\frac{n-1}{k-1}\right)+\frac{1}{2(n-1)}-\frac{1}{2(k-1)}+O!\left(\frac{1}{n^2}\right),$$
   hence
   $$P_k(n)
   =\frac{k}{n}\log!\left(\frac{n-1}{k-1}\right)+O!\left(\frac{1}{n}\right)
   \approx p\log!\left(\frac{1}{p}\right). $$
   The limiting objective is therefore the smooth function
   $$\Phi(p):=p\log!\left(\frac{1}{p}\right)=-p\log p, \qquad p\in(0,1).$$

3. **Proof by Optimization**:
   **(A) Continuous optimization of the limit.** Differentiate $\Phi$:
   $$\Phi'(p)=-(\log p+1),\qquad \Phi''(p)=-\frac{1}{p}<0.$$
   So $\Phi$ is strictly concave and has a unique maximizer where $\Phi'(p)=0$, i.e.
   $$\log p=-1 \quad\Longleftrightarrow\quad p=\frac{1}{e}.$$
   The maximal limiting success probability is
   $$\Phi!\left(\frac{1}{e}\right)=\frac{1}{e}.$$

   **(B) Exact finite- $n$ characterization (discrete optimality).** The cutoff strategy family is not just asymptotically optimal; it contains the exact optimal rule for each finite $n$. The key is a one-step dynamic-programming comparison at record times.

   Suppose at time $t$ you see a record. If you stop now, you win iff the global maximum is at position $t$. Since $\mathbb{P}(t\text{ is a record})=1/t$ and $\mathbb{P}(J=t)=1/n$, Bayes’ rule gives:
   $$\mathbb{P}(J=t\mid t\text{ is a record})=\frac{\mathbb{P}(J=t)}{\mathbb{P}(t\text{ is a record})}=\frac{1/n}{1/t}=\frac{t}{n}.$$
   So “stop at a record at time $t$” yields success probability $t/n$.

   If instead you continue past time $t$ (having rejected a record), the state of the problem at that decision point is fully summarized by the time index $t$: for the objective of selecting the overall maximum, any winning selection must occur at a future record time, and by the distribution-free rank symmetry the optimal continuation policy depends only on how many observations remain, not on the realized payoff level. In particular, the best achievable continuation win probability equals the success probability of the cutoff-record strategy with cutoff $t$:
$$C_t=\frac{t}{n}\Bigl(H_{n-1}-H_{t-1}\Bigr)=\frac{t}{n}\sum_{m=t}^{n-1}\frac{1}{m}.$$
Therefore, at a record time $t$ the optimal decision is:
$$\text{stop} \iff \frac{t}{n}\ge C_t
\iff 1 \ge \sum_{m=t}^{n-1}\frac{1}{m}
\iff H_{n-1}-H_{t-1}\le 1.$$
The tail sum $\sum_{m=t}^{n-1}\frac{1}{m}$ strictly decreases with $t$, so there is a unique threshold index
$$k^\star := \min\{t \in \{1,\dots,n-1\} : H_{n-1} - H_{t-1} \le 1\}.$$
such that it is optimal to reject all records up to time $k^\star$ and then accept the first record after $k^\star$. Using $H_{n-1}-H_{t-1}\approx \log\!\bigl(\frac{n}{t}\bigr)$, this condition is asymptotically $\log(n/k^\star)\approx 1$.

#### Generalization

To explore generalized stopping strategies, define a threshold proportion $p \in (0, 1)$ and set $k=\lfloor p\cdot n\rfloor$. The cutoff-record strategy family has an exact success probability
$$P(p;n):=P_{\lfloor pn\rfloor}(n)=\frac{\lfloor pn\rfloor}{n}\Bigl(H_{n-1}-H_{\lfloor pn\rfloor-1}\Bigr),$$ and the large- $n$ limit
$$\lim_{n\to\infty}P(p;n)=p\log!\left(\frac{1}{p}\right),$$
which is uniquely maximized at $p=1/e$ for the objective “maximize probability of selecting the overall maximum.”

If, instead of maximizing win probability, you study the **expected selected payoff** $\mathbb{E}[X_T]$, the mathematics changes: the objective now depends on the distribution $F$ (unlike the probability-of-best objective). For i.i.d. payoffs and a finite horizon, the optimal expected-payoff policy is characterized by a backward recursion (a finite-horizon optimal stopping dynamic program). Define the value function for $m$ remaining observations:
$$V_m := \sup_{T\in{1,\dots,m}}\mathbb{E}[X_T],$$
with boundary condition $V_1=\mathbb{E}[X]$. Then the Bellman recursion is
$$V_m=\mathbb{E}!\left[\max\bigl(X, V_{m-1}\bigr)\right],\qquad m\ge 2,$$
where $X\sim F$. This implies a time-dependent threshold rule: with $m$ observations remaining, stop iff
$$X\ge V_{m-1}.$$
Equivalently, in original time $t$, stop at time $t$ iff $X_t \ge V_{n-t}$, which can be computed numerically once $F$ is specified. This provides a principled way to simulate and compare “probability-of-best” strategies (distribution-free cutoff rules) versus “maximize expected payoff” strategies (distribution-dependent value-threshold rules).

---

## Folder Structure

```
project-root/
├── scripts/
│   ├── 01_data_generator.py      # Generates simulated payoff data
│   ├── 02_simulation.py   # Runs stopping strategy simulations
│   ├── 03_analysis.py            # Analyzes simulation results
│   ├── 04_visualization.py       # Visualizes results with graphs
├── outputs/                      # Stores generated outputs
│   ├── simulations.csv           # Simulation results
│   ├── analysis_summary.txt      # Summary of analysis
│   ├── graphs/                   # Visualization files
├── requirements.txt              # Python dependencies
└── README.md                     # Project documentation
```

---

## Usage

### 1. Setup the Project:

* Clone the repository.
* Ensure you have Python installed.
* Install required dependencies using the `requirements.txt` file:

```bash
pip install -r requirements.txt
```

### 2. Generate Payoff Data:

Run the script to generate payoff values for the simulation. Specify the number of payoffs, mean, standard deviation, and output path.

```bash
python scripts/01_data_generator.py --n 100 --mean 50 --stddev 10 --seed 42 --output outputs/payoffs.csv
```

### 3. Simulate Stopping Strategy:

Run the simulation using the generated data. Specify the input file, stopping threshold, and output file for results.

```bash
python scripts/02_simulation.py --input outputs/payoffs.csv --threshold 0.37 --output outputs/simulations.csv
```

### 4. Analyze Results:

Analyze the simulation results to compute summary statistics (mean, median, standard deviation) and optionally save them to a file.

```bash
python scripts/03_analysis.py --input outputs/simulations.csv --output outputs/analysis_summary.txt
```

### 5. Visualize Results:

Generate visualizations (e.g., payoff distributions) from the simulation data and save them as images.

```bash
python scripts/04_visualization.py --input outputs/simulations.csv --output outputs/graphs/payoff_distribution.png
```

---

## Requirements

* Python 3.8+
* Required libraries:

  * matplotlib

Install all dependencies with:

```bash
pip install -r requirements.txt
```
